{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b96adf5c-4630-4f06-b257-14e72e19c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.document_loaders import OnlinePDFLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60356923-0571-460a-a8f1-df140e1d785f",
   "metadata": {},
   "source": [
    "# Load the pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53023d76-8c3c-40f7-8361-238758a02d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"/path/to/pdf\"\n",
    "\n",
    "# Local PDF file uploads\n",
    "if local_path:\n",
    "  loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "  data = loader.load()\n",
    "else:\n",
    "  print(\"Upload a PDF file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1ce7bd-41ef-4b1f-adbe-ffafb8ac942e",
   "metadata": {},
   "source": [
    "## Load the model using LLamaCPP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2add898-5e4b-4b40-b8cd-3b6d0d40877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/path/to/gguf/model.gguf\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2abacf2a-b2c8-4a6d-8705-2534a4324c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "#You can download model file from HuggingFace or u can copy the model file from ollama models path then change the name to .gguf \n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path, verbose=False,n_ctx=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "839a8023-56e7-4926-8858-a6480b8637af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and chunk \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4096, chunk_overlap=64)\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83de2323-7a88-41d7-8ade-f3ff8c754ef0",
   "metadata": {},
   "source": [
    "# Embedding and Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69f1b338-c658-4bde-a524-07dba76c72de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibrahim/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 566 ms, sys: 210 ms, total: 776 ms\n",
      "Wall time: 5.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load embedding model \n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_llm = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0ba5048-ea2f-4afe-abf7-01cb8565e1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.36 s, sys: 406 ms, total: 6.77 s\n",
      "Wall time: 3.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vector_db_Chroma = Chroma.from_documents(\n",
    "    documents=chunks, \n",
    "                    embedding=embedding_llm,\n",
    "    collection_name=\"ragdb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e1c5d-dd99-48a7-95ec-703c4cd54aea",
   "metadata": {},
   "source": [
    "# Prompt and Response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "571ddb9e-9211-49ed-b61c-e50d32f2c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context then summarize the context to get the answer and If the answer is not contained in the context, say 'NO ANSWER IS AVAILABLE'\n",
    "The following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_Template = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt_Template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3881a74b-cdb0-465e-8877-60e7d447b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stream the output from llm \n",
    "def stream_output(chain,question):\n",
    "    stream = chain.stream(question)\n",
    "    for response in stream:\n",
    "        print(response, end='')  # Streamed response piece by piece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d7a36-aa2d-47e5-b793-287e416b9796",
   "metadata": {},
   "source": [
    "# Exmaples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6665e2e6-ff1b-4d0f-b979-f58d303b2c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "\n",
      "The types of audio deepfakes attacks are as follows: \n",
      "\n",
      "* **Imitation-based:** This method involves transforming secret audio (the original) to sound like another speech (target audio). It can be done by using human voices with similar tones and inflections or by masking algorithms. For instance, Efficient Wavelet Mask (EWM) is an algorithm that transforms the signal of the original audio to mimic the target audio. \n",
      "* **Synthetic-based:** This technique aims to transform text into natural speech in real time through a TTS (Text-to-Speech) system.  The process involves using pre-trained models, such as Tactoran 2, Deep Voice 3, and FastSpeech 2 to generate synthetic audio. These models are trained on large datasets of clean recordings and use their knowledge to produce high-quality results.\n",
      "* **Replay-based:** This is the type of attack that involves replaying a recording of the target speaker's voice and mimicking it through various techniques like far-field detection or cut-and-paste detection. \n",
      "\n",
      "**Summary:** The provided text describes three main types of audio deepfake attacks: imitation-based, synthetic-based, and replay-based. Each method hasCPU times: user 19min 46s, sys: 3.01 s, total: 19min 49s\n",
      "Wall time: 5min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"What is types of Audio  Deepfake Deepfake Attacks\"\n",
    "similar_docs = vector_db_Chroma.similarity_search(question, k=3) # get similer docments form vdb\n",
    "retriever = [ chunk.page_content for chunk in similar_docs ]\n",
    "stream_output(chain , {'context': retriever ,\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8b04d27-2954-41d3-8b26-020195589301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The context provided details several challenges in the field of Audio Deepfakes (AD). Here's a breakdown:\n",
      "\n",
      "**1. Limited AD Detection Methods for Non-English Languages:** Most existing research focuses on English-speaking voices, neglecting other languages like Arabic. This poses a significant challenge in developing robust AD detection methods that can effectively identify and classify fakeness across various linguistic backgrounds.\n",
      "**2. Lack of Accent Assessment in Existing AD Detection Methods:**  Current AD detection methods primarily rely on identifying the type of fake itself without considering nuances related to accent, tone, and other factors affecting audio authenticity. This approach limits their overall accuracy and effectiveness in accurately detecting real versus fake audio.\n",
      "\n",
      "These are just two major challenges highlighted in the text. The document also mentions the need for improved data collection, better model training techniques, and more comprehensive evaluation methods to address these limitations.CPU times: user 27min 7s, sys: 4.03 s, total: 27min 12s\n",
      "Wall time: 7min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"What is types Challenges in this research?\"\n",
    "similar_docs = vector_db_Chroma.similarity_search(question, k=3) # get similer docments form vdb\n",
    "retriever = [ chunk.page_content for chunk in similar_docs ]\n",
    "stream_output(chain , {'context': retriever ,\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c75ac90-5d59-42c8-a7bb-a44a539b3bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Answer:**\n",
      "\n",
      "Based on the provided text, here are the datasets sorted by the types of fake audio they address: \n",
      "\n",
      "* **M-AILABS Speech:** A German audio dataset designed for speech recognition and synthetic audio, containing real samples of varying lengths.\n",
      "* **Baidu Silicon Valley AI Lab cloned audio:**  Generated from a neural voice cloning tool, it contains high-quality multi-speaker audio clips in various formats.\n",
      "* **Fake or Real (FoR):** A dataset released in 2019 that includes 8 synthetic voices generated by DeepVoice3 and Google-WavNet, providing data for detecting fake audio samples in multiple formats (MP3, WAV).\n",
      "* **Ar-DAD Arabic Diversified Audio:** A dataset focused on Arabic speakers' voices. It contains both real and imitated voices from the Quran reciters. \n",
      "* **H-Voice:** This dataset is based on imitation and synthetic voices speaking in various languages such as Spanish, English, Portuguese, French, and Tagalog. \n",
      "* **ASV Spoof 2021 Challenge Dataset:** A publicly available dataset that contains audio samples for detection of spoofing attacks.\n",
      "\n",
      "\n",
      "**Please Note:** The text mentions that AD (Audio Detection) datasetsCPU times: user 27min 56s, sys: 3.91 s, total: 28min\n",
      "Wall time: 8min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Sort the Datasets for Fake Audio Detection \"\n",
    "similar_docs = vector_db_Chroma.similarity_search(question, k=3) # get similer docments form vdb\n",
    "retriever = [ chunk.page_content for chunk in similar_docs ]\n",
    "stream_output(chain , {'context': retriever ,\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdd26dc-815f-4ba6-9f6c-d389ad88088d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
