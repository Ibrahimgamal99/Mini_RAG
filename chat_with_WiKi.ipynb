{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_core langchain_text_splitters langchain_community wikipedia chromadb"
      ],
      "metadata": {
        "id": "SP-54Fq34AbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Ollama and download the LLM model and embeddding model"
      ],
      "metadata": {
        "id": "-Z9cTp32D19Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNdRfnul35sD",
        "outputId": "a64ce833-3ab2-4b18-d403-67e690ca4a78"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &\n",
        "!sleep 5 && tail /content/nohup.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d37PVaD84fMP",
        "outputId": "6bc5d784-5dee-4d44-bdb6-8353b2a0011c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "time=2024-09-28T16:06:36.615Z level=INFO source=download.go:175 msg=\"downloading 970aa74c0a90 in 3 100 MB part(s)\"\n",
            "time=2024-09-28T16:06:41.209Z level=INFO source=download.go:175 msg=\"downloading c71d239df917 in 1 11 KB part(s)\"\n",
            "time=2024-09-28T16:06:43.752Z level=INFO source=download.go:175 msg=\"downloading ce4a164fc046 in 1 17 B part(s)\"\n",
            "time=2024-09-28T16:06:46.366Z level=INFO source=download.go:175 msg=\"downloading 31df23ea7daa in 1 420 B part(s)\"\n",
            "[GIN] 2024/09/28 - 16:06:48 | 200 | 14.495674682s |       127.0.0.1 | POST     \"/api/pull\"\n",
            "2024/09/28 16:08:25 routes.go:1153: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
            "time=2024-09-28T16:08:25.979Z level=INFO source=images.go:753 msg=\"total blobs: 10\"\n",
            "time=2024-09-28T16:08:25.980Z level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\n",
            "time=2024-09-28T16:08:25.980Z level=INFO source=routes.go:1200 msg=\"Listening on 127.0.0.1:11434 (version 0.3.12)\"\n",
            "time=2024-09-28T16:08:25.981Z level=INFO source=common.go:135 msg=\"extracting embedded files\" dir=/tmp/ollama3092220900/runners\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ollama pull llama3.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uj6lFfp5MlP",
        "outputId": "b5591c56-75a0-4461-9cc6-425d7320ef43"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "success \u001b[?25h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ollama pull nomic-embed-text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sOJQLXn8B0g",
        "outputId": "470cf2c4-09bf-4c63-e363-cf7184dc787c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "success \u001b[?25h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpVuf82H3jyn",
        "outputId": "24d429df-6569-45c9-87f6-c987c95b42e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from langchain.text_splitter import NLTKTextSplitter\n",
        "from langchain.document_loaders import WikipediaLoader\n",
        "import nltk\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "llm = Ollama(model=\"llama3.2\") # Llama 3.2 3B"
      ],
      "metadata": {
        "id": "K7xQIF7W5bXp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Isaac Newton\"\n",
        "\n",
        "# ==============\n",
        "docs = WikipediaLoader(query=query, load_max_docs=1,\n",
        "                         doc_content_chars_max=50_0000).load()\n",
        "\n",
        "text_document = docs[0].page_content"
      ],
      "metadata": {
        "id": "wu-3eGPt65wf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [ text_document]\n",
        "metadatas = [{\"document\":query}]"
      ],
      "metadata": {
        "id": "8YY5pnKH7Xel"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = NLTKTextSplitter(\n",
        "    chunk_size=4096,\n",
        "    chunk_overlap=32\n",
        ") #This method splits the input documents into chunks , ensuring that there is an overlap of 32 characters between adjacent chunks.\n",
        "tokens_chunks = text_splitter.create_documents(\n",
        "    documents, metadatas=metadatas\n",
        ")# passing the input documents and potentially the metadata for embedding model"
      ],
      "metadata": {
        "id": "CcsCdNbA7oAl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create vector Database**\n"
      ],
      "metadata": {
        "id": "VkATqqV3Fy8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "vector_db_Chroma = Chroma.from_documents(\n",
        "                    documents=tokens_chunks,\n",
        "                    embedding=OllamaEmbeddings(model=\"nomic-embed-text\",show_progress=True), # embeding model\n",
        "                    collection_name=\"rag\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQITgBGj7qLJ",
        "outputId": "0ed145cd-b505-4cff-ebee-4cd677778ecf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OllamaEmbeddings: 100%|██████████| 13/13 [00:03<00:00,  3.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 603 ms, sys: 107 ms, total: 711 ms\n",
            "Wall time: 4.61 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Prompt Template"
      ],
      "metadata": {
        "id": "KsU4YidIG1OZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the similar docments using llm\n",
        "QUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"You are an AI language model assistant. Your task is to generate three\n",
        "    different versions of the given user question to retrieve relevant documents from\n",
        "    a vector database. By generating multiple perspectives on the user question, your\n",
        "    goal is to help the user overcome some of the limitations of the distance-based\n",
        "    similarity search. Provide these alternative questions separated by newlines.\n",
        "    Original question: {question}\"\"\",\n",
        ")\n",
        "retriever = MultiQueryRetriever.from_llm(\n",
        "    vector_db_Chroma.as_retriever(),\n",
        "    llm,\n",
        "    prompt=QUERY_PROMPT\n",
        ")"
      ],
      "metadata": {
        "id": "cmNFDgDN7xIV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG prompt\n",
        "template = \"\"\"Answer the question based ONLY on the following context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "f6-sRKJo8cEp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline takes a question as input. It retrieves relevant context using the retriever, formats the question and context using the prompt template, feeds the formatted prompt to the language model, and finally parses the model's response into a string."
      ],
      "metadata": {
        "id": "eafmL4kkXYh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "zEcBkaEP8kyk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_output(chain,question):\n",
        "    stream = chain.stream(question)\n",
        "    for response in stream:\n",
        "        print(response, end='')  # Streamed response piece by piece"
      ],
      "metadata": {
        "id": "KKgEkIt18UZJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example prompt\n",
        "question = \"In which year Newton die?\"\n",
        "stream_output(full_chain,question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh1RT_2g8nBz",
        "outputId": "3ec9f120-a855-45c9-e96b-101dc2780e56"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
            "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 18.82it/s]\n",
            "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 10.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The question \"In which year Newton die?\" is incomplete, as it doesn't specify whether it's referring to Isaac Newton or another person named Newton. However, I'll provide an answer based on the context of the text.\n",
            "\n",
            "Isaac Newton passed away in 1727."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Was Newton ever marrid \"\n",
        "stream_output(full_chain,question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTbLSFLc8pG5",
        "outputId": "cf8d829f-2996-43ef-fdb2-266f18f3b5db"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 18.75it/s]\n",
            "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 10.87it/s]\n",
            "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 18.90it/s]\n",
            "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  7.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No, according to the text, Isaac Newton was a bachelor, meaning he never married."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"tell me about gravity and summarize it into 3 points\"\n",
        "stream_output(full_chain,question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rNcqRrg82zS",
        "outputId": "b33a3b1f-5a8b-40d7-eea4-78cd441625e9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 19.22it/s]\n",
            "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 10.81it/s]\n",
            "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 10.89it/s]\n",
            "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 18.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gravity is a fundamental force of nature that was first discovered by Sir Isaac Newton. Here's a summary of his work on gravity in three key points:\n",
            "\n",
            "**Point 1: The Law of Universal Gravitation**\n",
            "Newton formulated the Law of Universal Gravitation, which states that every point mass attracts every other point mass with a force proportional to the product of their masses and inversely proportional to the square of the distance between them. This law applies to all objects in the universe, regardless of their size or composition.\n",
            "\n",
            "**Point 2: The Apple Incident**\n",
            "According to Newton's own account, he was inspired to formulate his theory of gravity after observing an apple falling from a tree. He asked himself why the apple always fell perpendicularly to the ground, rather than going sideways or upwards. This simple observation led him to develop his law of universal gravitation.\n",
            "\n",
            "**Point 3: Gravity is not just a force that pulls objects towards each other**\n",
            "Newton's theory of gravity also addressed the nature of the force itself. He believed that gravity was a property of space and time, rather than a force that acts between objects. In other words, gravity is not just a pull or tug on objects, but an inherent aspect of the fabric of spacetime itself. This idea laid the foundation for Newton's work on optics and his development of calculus."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Another Way To retrive using similarity search"
      ],
      "metadata": {
        "id": "N8OS45MwDAQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Was Newton ever marrid \"\n",
        "similar_docs = vector_db_Chroma.similarity_search(question, k=5)\n",
        "retriever = [ chunk.page_content for chunk in similar_docs ]\n",
        "# Example prompt\n",
        "#question = \"In which year Newton die?\"\n",
        "stream_output(chain , {'context': retriever ,\"question\": question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp6Sbw1F9Lrp",
        "outputId": "6362ae1c-6636-4038-db0c-37cd05283ef0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 10.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is no mention of Newton being married in the provided text."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jtbtymnDdO5S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}